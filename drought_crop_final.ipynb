{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drought and Crop Yield <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "\n",
    "#### Project Proposal:\n",
    "Throughout the years, droughts have received more attention especially by weather specialists. With climate change being actively tracked, it is important for us to understand drought patterns and how they will relate to crop yield, which will help us understand global food security for the coming years. Recently, there has been a growing food demand with an increase in the world population along with drastic changes in the weather patterns. Through this study, we plan to help farmers understand the extreme changes within weather patterns and how it will impact crop yield by understanding previous crop yield and loss patterns. \n",
    "\n",
    "\n",
    "#### Proposed Project: \n",
    "We will understand the relationship trend between crop yield and drought pattern in North and South America, and try to comprehend which crops are less correlated to drought data indications. Throughout this project, we will want to get a better knowledge of drought impact on agriculture and to extend further, how crops are currently affected by climate change. \n",
    "\n",
    "#### Questions to answer:\n",
    "* Which crops are less correlated to drought data indications? If we have some crops that are less correlated, will this crop be a ‘good-yield’ crop in the areas that are more prone to drought? \n",
    "* How has climate change impacted crop yields over the last few years and analyzing whether there were any extreme changes within the US crop yield patterns?\n",
    "* As an addition, we would also want to understand the soil moisture data in correspondence to drought patterns and how soil moisture affects crop yields.\n",
    "\n",
    "#### Scope of Study:\n",
    "* Location: We will study this crop yield and drought indication relation across US states\n",
    "* Timeframe: according to limitation of data, We will need to focus on study of data from year 2010 to 2020\n",
    "\n",
    "#### Limitations: \n",
    "Analyzing crop yield data requires tons and tons of data such as soil moisture, temperature, use of fertilizers. For better analysis and performance, it is important to obtain as much data as possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNJvQMZTCEGu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import glob\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "import altair as alt\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib inline\n",
    "# import ee\n",
    "\n",
    "# set max df column display\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlqFthKKCEGv"
   },
   "source": [
    "### Primary Dataset <a class=\"anchor\" id=\"primary\"></a>\n",
    "\n",
    "our crop dataset from https://www.nass.usda.gov/Statistics_by_Subject/index.php?sector=CROPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlJP6OnzCEGv"
   },
   "outputs": [],
   "source": [
    "# get all the data together\n",
    "def getframe(folderpath, axis=0):\n",
    "    path = folderpath\n",
    "    all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "    concat_df   = pd.concat(df_from_each_file, axis=axis, ignore_index=True)\n",
    "    return concat_df\n",
    "\n",
    "# crop_df = getframe('drive/MyDrive/Colab Notebooks/data/crop_data/')\n",
    "crop_df = getframe('crops_datasets_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "ZiteWOdgCEGw",
    "outputId": "879ebc39-4e80-440b-a089-0098f4018282"
   },
   "outputs": [],
   "source": [
    "crop_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaIm4q_6CEGx"
   },
   "source": [
    "### Secondary Dataset <a class=\"anchor\" id=\"secondary_dataset\"></a>\n",
    "#### Now let's get our drought data from USA Drought Monitor website\n",
    "\n",
    "Source: https://droughtmonitor.unl.edu/DmData/DataDownload/ComprehensiveStatistics.aspx \n",
    "\n",
    "The file will be loaded in a csv format from the above website using the following parameters:\n",
    "\n",
    "* Start Date: 01/01/2010 and End Date: 12/31/2020\n",
    "* Spatial Scale: State and choose all states\n",
    "* Statistics Category: Reports at percent level one drought category at time\n",
    "\n",
    "To understand the data better, we could use the data dictionary present in the website itself: \n",
    "*   None is no reported drought\n",
    "*   D0 - Abnormally dry\n",
    "*   D1 - Moderate Drought\n",
    "*   D2 - Severe Drought\n",
    "*   D3 - Extreme Drought\n",
    "*   D4 - Exceptional Drought.\n",
    "\n",
    "These drought variables are calculated using various data such as precipitation, soil moisuture, surface temperature being the main variables.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-BEoS4zCEGx"
   },
   "outputs": [],
   "source": [
    "# drought_df_raw = pd.read_csv('drive/MyDrive/Colab Notebooks/data/drought_data/drought_data_2010-2020.csv')\n",
    "drought_df_raw = pd.read_csv('secondary_dataset/drought_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2aS9XSzCEGy"
   },
   "outputs": [],
   "source": [
    "# get date to Datetime format\n",
    "# find dict for state abbreviation to lookup key and value then merge with the crop file\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\", \"California\": \"CA\", \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\", \"Delaware\": \"DE\", \"Florida\": \"FL\", \"Georgia\": \"GA\", \"Hawaii\": \"HI\", \"Idaho\": \"ID\", \"Illinois\": \"IL\", \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\", \"Kansas\": \"KS\", \"Kentucky\": \"KY\", \"Louisiana\": \"LA\", \"Maine\": \"ME\", \"Maryland\": \"MD\", \"Massachusetts\": \"MA\", \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\", \"Mississippi\": \"MS\", \"Missouri\": \"MO\", \"Montana\": \"MT\", \"Nebraska\": \"NE\", \"Nevada\": \"NV\", \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\", \"New Mexico\": \"NM\", \"New York\": \"NY\", \"North Carolina\": \"NC\", \"North Dakota\": \"ND\", \"Ohio\": \"OH\", \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\", \"Pennsylvania\": \"PA\", \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\", \"South Dakota\": \"SD\", \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\", \"Utah\": \"UT\", \"Vermont\": \"VT\", \"Virginia\": \"VA\", \"Washington\": \"WA\", \"West Virginia\": \"WV\", \"Wisconsin\": \"WI\", \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\", \"American Samoa\": \"AS\", \"Guam\": \"GU\", \"Northern Mariana Islands\": \"MP\", \"Puerto Rico\": \"PR\",\n",
    "     \"United States Minor Outlying Islands\": \"UM\", \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "    \n",
    "# invert the dictionary\n",
    "abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))\n",
    "\n",
    "drought_df_raw['state'] = drought_df_raw['StateAbbreviation'].map(abbrev_to_us_state)\n",
    "drought_df_raw['MapDate'] = pd.to_datetime(drought_df_raw['MapDate'], format='%Y%M%d')\n",
    "drought_df_raw.set_index('MapDate', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMxi_rsBCEGy"
   },
   "outputs": [],
   "source": [
    "drought_year_df = drought_df_raw.groupby([pd.Grouper(freq='Y'), 'state']).mean()\n",
    "drought_year_df = drought_year_df.reset_index()\n",
    "drought_year_df['Year'] = drought_year_df['MapDate'].dt.year\n",
    "drought_year_df.rename(columns={'state':'State'}, inplace=True)\n",
    "drought_year_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6rx4PsQCEGz"
   },
   "source": [
    "#### Precipitation by State from 2010-2022\n",
    "Source : https://www.ncdc.noaa.gov/cag/statewide/time-series\n",
    "\n",
    "* First get the data from the source above and concat them together.\n",
    "* The data that we get for this project is monthly data by state from year 2010 to year 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "lx77WMShCEGz",
    "outputId": "d510fde3-653a-4ece-fc95-4b092c09397e"
   },
   "outputs": [],
   "source": [
    "# read each file and change column value to each state\n",
    "import ntpath\n",
    "\n",
    "# path = 'drive/MyDrive/Colab Notebooks/data/precipitation/'\n",
    "path = 'secondary_dataset/precipitation'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "files_list = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file, skiprows=4)\n",
    "    y = file.rsplit('.', 1)\n",
    "    x = y[0].rsplit('pcp_', 1)\n",
    "    df['state'] = x[-1]\n",
    "    files_list.append(df)\n",
    "pcp_df = pd.concat(files_list, axis=0, ignore_index=True)\n",
    "pcp_df['Date'] = pd.to_datetime(pcp_df.Date, format='%Y%m')\n",
    "pcp_df.set_index('Date', inplace=True)\n",
    "pcp_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "oPB7IRaXCEGz",
    "outputId": "88fbcea2-a70c-4e5f-a499-9e87a1293bd3"
   },
   "outputs": [],
   "source": [
    "# pcp_df by year average value\n",
    "pcp_year_df = pcp_df.groupby([pd.Grouper(freq='Y'), 'state']).mean()\n",
    "pcp_year_df = pcp_year_df.reset_index()\n",
    "pcp_year_df['Year'] =pcp_year_df['Date'].dt.year\n",
    "pcp_year_df.rename(columns={'Value':'precip', 'state':'State'}, inplace=True)\n",
    "pcp_year_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDAirnFiCEGz"
   },
   "source": [
    "#### Land Temperature by State\n",
    "Source : https://www.ncdc.noaa.gov/cag/statewide/time-series\n",
    "\n",
    "* First get the data from the source above and concat them together.\n",
    "* The data that we get for this project is monthly data by state from year 2010 to year 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Sv-fnP1UCEG0",
    "outputId": "b3b176b5-2d0f-4355-ebc9-6804dab5945f"
   },
   "outputs": [],
   "source": [
    "# read and write csv to collect state from the top of the cell\n",
    "# path = 'drive/MyDrive/Colab Notebooks/data/temperature/'\n",
    "path = 'secondary_dataset/temperature'\n",
    "files = [os.path.join(path, f) for f in os.listdir(path)] # if os.path.isfile(os.path.join(path, f))\n",
    "df_list = []\n",
    "for f in files:\n",
    "    file = open(f, 'r')\n",
    "    firstline = file.readline()\n",
    "    state = firstline.split(',')[0] # extract state from csv file\n",
    "    df = pd.read_csv(f, skiprows=4) # read_csv to df \n",
    "    df['state'] = state\n",
    "    df_list.append(df)\n",
    "temp_df = pd.concat(df_list, ignore_index=True)\n",
    "temp_df['Date'] = pd.to_datetime(temp_df['Date'], format='%Y%M')\n",
    "temp_df.set_index('Date', inplace=True)\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmofb9yhCEG0"
   },
   "outputs": [],
   "source": [
    "temp_year_df = temp_df.groupby([pd.Grouper(freq='Y'), 'state']).mean()\n",
    "temp_year_df = temp_year_df.reset_index()\n",
    "temp_year_df['Year'] =temp_year_df['Date'].dt.year\n",
    "temp_year_df.rename(columns={'Value':'temp', 'state':'State'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrO6fHuUCEG0"
   },
   "source": [
    "#### Palmer Drought Severity Index (PDSI)\n",
    "Source : https://www.ncdc.noaa.gov/cag/statewide/time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "UPoWa4c9CEG0",
    "outputId": "2ed4ff09-7dae-4838-aaad-806cb2414977"
   },
   "outputs": [],
   "source": [
    "# read and write csv to collect state from the top of the cell\n",
    "# path = 'drive/MyDrive/Colab Notebooks/data/pdsi'\n",
    "path = 'secondary_dataset/pdsi'\n",
    "files = [os.path.join(path, f) for f in os.listdir(path)] # if os.path.isfile(os.path.join(path, f))\n",
    "df_list = []\n",
    "for f in files:\n",
    "    file = open(f, 'r')\n",
    "    firstline = file.readline()\n",
    "    state = firstline.split(',')[0] # extract state from csv file\n",
    "    df = pd.read_csv(f, skiprows=3) # read_csv to df \n",
    "    df['state'] = state\n",
    "    df_list.append(df)\n",
    "pdsi_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "pdsi_df['Date'] = pd.to_datetime(pdsi_df['Date'], format='%Y%M')\n",
    "pdsi_df.set_index('Date', inplace=True)\n",
    "pdsi_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naxYiIu_CEG0"
   },
   "outputs": [],
   "source": [
    "pdsi_year_df = pdsi_df.groupby([pd.Grouper(freq='Y'), 'state']).mean()\n",
    "pdsi_year_df = pdsi_year_df.reset_index()\n",
    "pdsi_year_df['Year'] =pdsi_year_df['Date'].dt.year\n",
    "pdsi_year_df.rename(columns={'Value':'pdsi', 'state':'State'}, inplace=True)\n",
    "pdsi_year_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Bluxna0CEG0"
   },
   "source": [
    "## **Data Cleaning and Manipulation** \n",
    "\n",
    "**Step 1**\n",
    "\n",
    "* For our primary dataset from the crops, we first want to look at the top 10 commodoties in the U.S. We will need to visualize this data at a commodity level rather than at state-level. We will then get the top 10 commodities by filtering out by data-item on acres harvested or produced.\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "Create a bar graph for top 10 Commodities in altair with an interaction by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkcQGvEJCEG1"
   },
   "outputs": [],
   "source": [
    "def top_commodities():\n",
    "    filter1 = crop_df.copy()\n",
    "\n",
    "    filter1 = filter1.drop(columns = ['Program', 'Period','Week Ending','Geo Level','State ANSI','Ag District',\n",
    "                                    'Ag District Code','County','County ANSI','Zip Code','Region', 'watershed_code',\n",
    "                                    'Watershed','Domain','Domain Category', 'CV (%)'])\n",
    "\n",
    "    filter1 = filter1[filter1[\"Commodity\"].str.contains(\"CROP TOTALS\") == False]\n",
    "    filter1 = filter1[filter1[\"Commodity\"].str.contains(\"RENT\") == False]\n",
    "    commodity = filter1['Commodity'].unique() # Check to Remove commodities that are totals\n",
    "\n",
    "    filter1 = filter1[filter1[\"Data Item\"].str.contains(\"ACRES HARVESTED\") == True]\n",
    "    filter1 = filter1[filter1[\"Data Item\"].str.contains(\"EXCL ALFALFA\") == False]\n",
    "    data_items = filter1['Data Item'].unique() # Check to retain only acres harvested\n",
    "\n",
    "    # Convert Values to integer to add and remove any rows that\n",
    "    filter1['Value'] = filter1['Value'].str.replace(r'[()]',\"to_remove\")\n",
    "\n",
    "    # Remove any characters that are not numbers from the value column as it isn't needed\n",
    "    filter1_f = filter1[filter1[\"Value\"].str.contains(\"to_remove\") == True] ## Do not need\n",
    "    filter1_t = filter1[filter1[\"Value\"].str.contains(\"to_remove\") == False]\n",
    "\n",
    "    # Convert Value column into an integer\n",
    "    filter1_t['Value'] = filter1_t['Value'].str.replace(',', '')\n",
    "    filter1_t['Value'] = filter1_t['Value'].astype(str).astype(int)\n",
    "    filter1_t = filter1_t.reset_index(drop=True)\n",
    "\n",
    "    # Groupby commodity and sum the acres harvested in thousands\n",
    "    filter2 = filter1_t.groupby(['Commodity'])['Value'].mean().sort_values(ascending=False).reset_index()\n",
    "    filter2 = filter2.rename(columns = {'Value':'Average Acres Harvested (0000s)'})\n",
    "    filter2['Average Acres Harvested (0000s)'] = filter2['Average Acres Harvested (0000s)'].div(10000).round(2)\n",
    "\n",
    "    return filter2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "OhaAKNaeCEG1",
    "outputId": "962a2824-a61d-4d1d-9557-54b24eb2b172"
   },
   "outputs": [],
   "source": [
    "alt.themes.enable('fivethirtyeight')\n",
    "\n",
    "source = top_commodities()\n",
    "\n",
    "\n",
    "bar = alt.Chart(source, title='Top 10 Commodities within 2010 - 2020').mark_bar().encode(\n",
    "    x='Average Acres Harvested (0000s):Q',\n",
    "    y=alt.Y('Commodity:N', sort='-x')\n",
    ").transform_window(\n",
    "    rank='rank(Average Acres Harvested (0000s))',\n",
    "    sort=[alt.SortField('Average Acres Harvested (0000s)', order='descending')]\n",
    ").transform_filter(\n",
    "    (alt.datum.rank < 11)\n",
    ")\n",
    "\n",
    "bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmKvt3Z4CEG1"
   },
   "source": [
    "\n",
    "### Merging the Datasets\n",
    "\n",
    "We also wants to merge the crop data with all other variables that we have as well as making it a GeoDataframe for easy choropleth plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cBod83YCEG1"
   },
   "outputs": [],
   "source": [
    "# state shape file df\n",
    "# path = 'drive/MyDrive/Colab Notebooks/data/states_shapefile/cb_2018_us_state_500k.shx'\n",
    "path = 'states_shapefile/cb_2018_us_state_500k.shx'\n",
    "geo_gdf = gpd.read_file(path)\n",
    "geo_gdf['NAME'] = geo_gdf['NAME'].str.upper()\n",
    "geo_gdf.rename(columns={'NAME':'State'}, inplace=True)\n",
    "geo_gdf.State = geo_gdf.State.str.capitalize()\n",
    "geo_gdf = geo_gdf[['State', 'geometry']]\n",
    "geo_gdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQSW3mnBCEG1"
   },
   "source": [
    "#### Cleaning the Primary Crop Data into Acre, Value ($), and Weight Harvested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvCbdkdVCEG2"
   },
   "outputs": [],
   "source": [
    "# filter the crop_df1 to only acres_harvested items and only interested columns\n",
    "crop_df1 = crop_df.copy()\n",
    "crop_df1 = crop_df1[(crop_df1['Data Item'].str.contains('ACRES HARVESTED') == True)&(crop_df1['Year'] >= 2010)&(crop_df1['Year'] <= 2020)\\\n",
    "                    &(~crop_df1['Commodity'].str.contains('FIELD CROP TOTALS'))]\n",
    "crop_df1 = crop_df1[['Year', 'State', 'Commodity', 'Value']]\n",
    "crop_df1.Value = crop_df1.Value.apply(lambda x : x.replace(',', ''))\n",
    "crop_df1.Value = pd.to_numeric(crop_df1.Value, errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Pc0JlEZZCEG2",
    "outputId": "20ffb1c9-34ba-48a0-dde5-33820af31c16"
   },
   "outputs": [],
   "source": [
    "# group by crop and year, sum value\n",
    "crop_df1 = crop_df1.groupby(['Year', 'State', 'Commodity']).sum()\n",
    "crop_df1.reset_index(inplace=True)\n",
    "crop_df1.State = crop_df1.State.str.capitalize()\n",
    "crop_df1.rename(columns={'Value':'Acre'}, inplace=True)\n",
    "crop_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "9Eu4AYkJCEG2",
    "outputId": "2a2c708b-5c0f-4b38-afef-09b825961915"
   },
   "outputs": [],
   "source": [
    "# filter crop data item by production measured in $\n",
    "crop_df2 = crop_df.copy()\n",
    "crop_df2 = crop_df2[(crop_df2['Data Item'].str.contains('PRODUCTION, MEASURED IN \\$') == True) &(crop_df2['Year'] >= 2010)&(crop_df2['Year'] <= 2020)\\\n",
    "                   &(~crop_df2['Commodity'].str.contains('CROP TOTALS'))] \n",
    "crop_df2 = crop_df2[['Year', 'State', 'Commodity', 'Value']]\n",
    "crop_df2.Value = crop_df2.Value.apply(lambda x : x.replace(',', ''))\n",
    "crop_df2.Value = pd.to_numeric(crop_df2.Value, errors='coerce').fillna(0)\n",
    "crop_df2.rename(columns={'Value':'Value_$'}, inplace=True)\n",
    "crop_df2 = crop_df2.groupby(['Year', 'State', 'Commodity']).sum()\n",
    "crop_df2.reset_index(inplace=True)\n",
    "crop_df2.State = crop_df2.State.str.capitalize()\n",
    "crop_df2.rename(columns={'Value':'Acre'}, inplace=True)\n",
    "crop_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "GIUJVodzCEG2",
    "outputId": "d00f1a7a-b9b0-4086-e269-39319f73e587"
   },
   "outputs": [],
   "source": [
    "crop_df3 = crop_df.copy()\n",
    "\n",
    "# looking at what filter we would need to use and what different unit do we have that we need to transform into weight single unit\n",
    "crop_df3 = crop_df3[crop_df3['Data Item'].str.contains('PRODUCTION, MEASURED IN')]\n",
    "units = crop_df3['Data Item'].unique().tolist()\n",
    "new_unit = set([unit.split('-')[1].strip() for unit in units])\n",
    "new_unit\n",
    "\n",
    "unit_list = ['PRODUCTION, MEASURED IN 480 LB BALES',\n",
    "'PRODUCTION, MEASURED IN BU',\n",
    "'PRODUCTION, MEASURED IN CWT',\n",
    "'PRODUCTION, MEASURED IN GALLONS',\n",
    "'PRODUCTION, MEASURED IN LB',\n",
    "'PRODUCTION, MEASURED IN TONS']\n",
    "\n",
    "# first filter the unit above in the data items then convert the unit to tons\n",
    "# crop_df3 = crop_df3[crop_df3['Data Item'].str.contains(('|').join(unit_list))]\n",
    "crop_df3 = crop_df3[crop_df3['Data Item'].str.contains('PRODUCTION, MEASURED IN TONS')]\n",
    "crop_df3 = crop_df3[['Year', 'State', 'Commodity', 'Data Item', 'Value']]\n",
    "crop_df3.Value = crop_df3.Value.apply(lambda x : x.replace(',', ''))\n",
    "crop_df3.Value = pd.to_numeric(crop_df3.Value, errors='coerce').dropna()\n",
    "\n",
    "# crop_df3['Value'] = np.where(crop_df3['Data Item'].str.contains('PRODUCTION, MEASURED IN 480 LB BALES'), crop_df3['Value']*480/2000, crop_df3['Value']) #convert the unit into pounds then tons\n",
    "# crop_df3['Value'] = np.where(crop_df3['Data Item'].str.contains('PRODUCTION, MEASURED IN BU'), crop_df3['Value']*0.021772, crop_df3['Value']) \n",
    "# crop_df3['Value'] = np.where(crop_df3['Data Item'].str.contains('PRODUCTION, MEASURED IN CWT'), crop_df3['Value']*0.056, crop_df3['Value'])\n",
    "# crop_df3['Value'] = np.where(crop_df3['Data Item'].str.contains('PRODUCTION, MEASURED IN GALLONS'), crop_df3['Value']*11.358/2000, crop_df3['Value']) # convert syrub gallon to pounds then tons\n",
    "crop_df3 = crop_df3[['Year', 'State', 'Commodity', 'Value']]\n",
    "crop_df3.rename(columns={'Value':'Tons'}, inplace=True)\n",
    "crop_df3.State = crop_df3.State.str.capitalize()\n",
    "crop_df3 = crop_df3.groupby(['Year', 'State', 'Commodity']).mean().reset_index()\n",
    "crop_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4j113NaOCEG2",
    "outputId": "0ca0f844-5b36-4138-a497-4238c2b746a4"
   },
   "outputs": [],
   "source": [
    "# merge 2 crops value by Acre and by Value\n",
    "all_crop_df = pd.merge(crop_df1, crop_df2, on=['Year', 'State', 'Commodity'], how='outer')\n",
    "all_crop_df_value = pd.merge(all_crop_df, crop_df3, on=['Year', 'State', 'Commodity'], how='outer')\n",
    "all_crop_df_value = all_crop_df_value.dropna()\n",
    "all_crop_df_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging crop yield variables with drought variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "qlpk_eJSCEG2",
    "outputId": "943a79e7-e4a0-4afa-fa7f-7046e0fea0f4"
   },
   "outputs": [],
   "source": [
    "# merge crop and all variables on year and state\n",
    "dfs = [all_crop_df_value, drought_year_df, pdsi_year_df, pcp_year_df, temp_year_df]\n",
    "df = reduce(lambda left,right : pd.merge(left, right, on=['Year', 'State'], how='left'), dfs).fillna(0)\n",
    "df = df[['Year', 'State', 'Commodity', 'Value_$','Acre', 'Tons','None', 'D0', 'D1', 'D2', 'D3', 'D4', 'pdsi', 'precip', 'temp']]\n",
    "df['Year'] = pd.to_datetime(df.Year, format='%Y')\n",
    "df['Commodity'] = df['Commodity'].replace('HAY & HAYLAGE', 'HAY')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group states into regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "kb6q_NsPCEG2",
    "outputId": "312db3f3-1c7f-48f6-ac3c-fb91fc1ea3c1"
   },
   "outputs": [],
   "source": [
    "# create a geodataframe to work with geopandas or folium map\n",
    "new_gdf = pd.merge(df, geo_gdf, on='State', how='left')\n",
    "gdf = gpd.GeoDataFrame(new_gdf)\n",
    "gdf\n",
    "\n",
    "df_region = df.copy()\n",
    "df_region['State'].unique()\n",
    "\n",
    "df_region['Region']  = df_region['State'].replace(['Alabama','Florida','Georgia','South carolina'],'Southeast', inplace=True)\n",
    "df_region['Region']  = df_region['State'].replace(['Montana', 'Idaho', 'Wyoming', 'Nevada', 'Utah', 'Colorado', 'Arizona','New mexico'],'Mountain', inplace=True)\n",
    "df_region['Region']  = df_region['State'].replace(['Arkansas','Louisiana','Mississippi'],'Delta', inplace=True)\n",
    "df_region['Region']  = df_region['State'].replace(['California', 'Oregon','Washington'],'Pacific Coast', inplace=True)\n",
    "df_region['Region']  = df_region['State'].replace(['Maine','Vermont','New york','Massachusetts', 'Connecticut' , 'Rhode island', 'New hampshire', 'New jersey','Pennsylvania', 'Delaware', 'Maryland'],'Northeast', inplace=True)\n",
    "df_region['Region']  = df_region['State'].replace(['North dakota', 'South dakota', 'Nebraska', 'Kansas'],'Northern Plains', inplace=True)\n",
    "df_region['Region']  = df_region['State'].replace(['Kentucky','North carolina','Tennessee','Virginia','West virginia'],'Appalachian', inplace=True)\n",
    "df_region['Region']  = df_region['State'].replace(['Michigan', 'Minnesota', 'Wisconsin'],'Lake States', inplace=True)\n",
    "df_region['Region']  = df_region['State'].replace(['Texas','Oklahoma'],'Southern Plains', inplace=True)\n",
    "df_region['Region']  = df_region['State'].replace(['Illinois', 'Indiana', 'Iowa', 'Ohio','Missouri'], 'Corn Belt', inplace=True)\n",
    "\n",
    "df_region = df_region.drop(columns=['Region'])\n",
    "\n",
    "\n",
    "\n",
    "gdf_region = new_gdf.copy()\n",
    "gdf_region['Region']  = gdf_region['State'].replace(['Alabama','Florida','Georgia','South carolina'],'Southeast', inplace=True)\n",
    "gdf_region['Region']  = gdf_region['State'].replace(['Montana', 'Idaho', 'Wyoming', 'Nevada', 'Utah', 'Colorado', 'Arizona','New mexico'],'Mountain', inplace=True)\n",
    "gdf_region['Region']  = gdf_region['State'].replace(['Arkansas','Louisiana','Mississippi'],'Delta', inplace=True)\n",
    "gdf_region['Region']  = gdf_region['State'].replace(['California', 'Oregon','Washington'],'Pacific Coast', inplace=True)\n",
    "gdf_region['Region']  = gdf_region['State'].replace(['Maine','Vermont','New york','Massachusetts', 'Connecticut' , 'Rhode island', 'New hampshire', 'New jersey','Pennsylvania', 'Delaware', 'Maryland'],'Northeast', inplace=True)\n",
    "gdf_region['Region']  = gdf_region['State'].replace(['North dakota', 'South dakota', 'Nebraska', 'Kansas'],'Northern Plains', inplace=True)\n",
    "gdf_region['Region']  = gdf_region['State'].replace(['Kentucky','North carolina','Tennessee','Virginia','West virginia'],'Appalachian', inplace=True)\n",
    "gdf_region['Region']  = gdf_region['State'].replace(['Michigan', 'Minnesota', 'Wisconsin'],'Lake States', inplace=True)\n",
    "gdf_region['Region']  = gdf_region['State'].replace(['Texas','Oklahoma'],'Southern Plains', inplace=True)\n",
    "gdf_region['Region']  = gdf_region['State'].replace(['Illinois', 'Indiana', 'Iowa', 'Ohio','Missouri'], 'Corn Belt', inplace=True)\n",
    "\n",
    "gdf = gpd.GeoDataFrame(gdf_region)\n",
    "gdf.head(5)\n",
    "\n",
    "\n",
    "df_region_groupby = df_region.groupby(['Year','State'])['None','D0','D1','D2','D3','D4','pdsi','precip','temp'].mean().reset_index()\n",
    "df_region_groupby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwfzoTOSCEG3"
   },
   "source": [
    "## Crop and Drought Data Analysis\n",
    "<!-- What do we want to know from the analysis and how can we tell the story from those finding -->\n",
    "The questions we want answer are, how do crop response to different drought variables? What crop are more tolerance to drought relative to others? How would you re-clustering US state based on drought and climate data?\n",
    "\n",
    "1. let's first explore each drought variable in chart\n",
    "2. then let's explore top 5 crops in US small multiples choropleth over 5 years periods\n",
    "3. crops correlation with each variables\n",
    "4. US state clustering by drought and climate variables, we can work on monthly data of each varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by year and crops\n",
    "df_crops = df.copy()\n",
    "df_crops = df_crops.groupby(['Year', 'Commodity'])['Tons'].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpGXpY88CEG3"
   },
   "outputs": [],
   "source": [
    "# Let's take a peak at varibles vis but let's also plob in crops by states\n",
    "alt.data_transformers.disable_max_rows() # disable max row error\n",
    "\n",
    "# selection\n",
    "selection = alt.selection_multi(fields=['State'], bind='legend')\n",
    "\n",
    "#base altair with df\n",
    "base = alt.Chart(df_region_groupby)\n",
    "\n",
    "crop_norm = alt.Chart(df_crops).mark_area().encode(\n",
    "    x=\"Year:T\",\n",
    "    y=alt.Y(\"Tons:Q\", stack=\"normalize\"),\n",
    "    color=alt.Color(\"Commodity:N\", legend=alt.Legend(orient='right'))\n",
    ")\n",
    "\n",
    "crop_bar = alt.Chart(df_crops).mark_bar().encode(\n",
    "    x=\"Year:T\",\n",
    "    y=alt.Y(\"Tons:Q\"),\n",
    "    color=alt.Color(\"Commodity:N\", legend=alt.Legend(orient='right'))\n",
    ")\n",
    "\n",
    "pdsi = base.mark_line().encode(\n",
    "    x=alt.X('Year:T'),\n",
    "    y=alt.Y('pdsi:Q'),\n",
    "    color=alt.Color('State:N', legend=alt.Legend(orient='right')),\n",
    "    tooltip=['State','D4']\n",
    ").add_selection(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "e_ft1q7HCEG3",
    "outputId": "b3df5678-c8cf-4798-84f5-af95048c4273"
   },
   "outputs": [],
   "source": [
    "# Let's see if we can average variables across states\n",
    "# all the variable group only by year and average the results\n",
    "avg_df = df.groupby('Year').mean().reset_index()\n",
    "corr_df = avg_df.corr()\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ov0Z9EsXCEG3"
   },
   "outputs": [],
   "source": [
    "def corrmatrix(df, method='pearson'):\n",
    "    # corr_df_stack = df.corr(method).stack().reset_index().rename(columns={0: 'correlation', 'level_0': 'variable', 'level_1': 'variable2'})\n",
    "    corr_df_stack = df.corr(method).reset_index().melt('index').rename(columns={'index': 'variable', 'variable': 'variable2', 'value': 'correlation'})\n",
    "    corr_df_stack['correlation_label'] = corr_df_stack['correlation'].map('{:.2f}'.format).fillna(0)\n",
    "    sort=df.columns.tolist() \n",
    "    # create altair matrix\n",
    "    base = alt.Chart(corr_df_stack).encode(\n",
    "        x=alt.X('variable:O', sort=sort, title=None, axis=alt.Axis(labelFontSize=16)),\n",
    "        y=alt.Y('variable2:O', sort=sort, title=None, axis=alt.Axis(labelFontSize=16))\n",
    "    )\n",
    "\n",
    "    # text\n",
    "    text = base.mark_text(size=14).encode(\n",
    "        text='correlation_label:N',\n",
    "        color=alt.condition(alt.datum.correlation > 0.5, alt.value('white'), alt.value('black'))\n",
    "    )\n",
    "\n",
    "    # heatmap\n",
    "    hm = base.mark_rect().encode(\n",
    "        color='correlation:Q'\n",
    "    )\n",
    "\n",
    "    corr_chart = (hm + text).properties(\n",
    "        width=600,\n",
    "        height=600\n",
    "    )\n",
    "    return corr_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 903
    },
    "id": "NfuF9kwKCEG3",
    "outputId": "b4ec9b13-0e92-48d0-ef4c-d27cdbe26583"
   },
   "outputs": [],
   "source": [
    "corrmatrix(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMuwbvQCCEG3"
   },
   "outputs": [],
   "source": [
    "# let's look at the pairplot with seaborn\n",
    "df_pairplot = df[['Value_$', 'Acre', 'Tons', 'None', 'D0', 'D1', 'D2', 'D3', 'D4', 'pdsi', 'precip', 'temp']]\n",
    "# sns.pairplot(df_pairplot, kind ='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Srj5eKtTCEG3",
    "outputId": "0a188456-34b9-4d83-c39a-cbde4a730945"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data = df_pairplot,\n",
    "             y_vars=['None', 'D0', 'D1', 'D2', 'D3', 'D4', 'pdsi', 'precip', 'temp'],\n",
    "             x_vars=['Value_$','Acre','Tons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXDcbeXaCEG4"
   },
   "source": [
    "### What crops are less affected by drought?\n",
    "As we can now see the relationship of all the variables, now we can see which crop variable would be a good representation of the yield. Here we will use crop production yield in weight to compare among different crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "q17iYfVvCEG4",
    "outputId": "fcfa08d2-e558-43b4-f71f-2532bf692911"
   },
   "outputs": [],
   "source": [
    "df['Commodity'] = df['Commodity'].replace('HAY & HAYLAGE', 'HAY')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "WQ_zmCztCEG4",
    "outputId": "7959399e-c8b1-4046-ba1e-1d1cf4f21517"
   },
   "outputs": [],
   "source": [
    "# compare crop yield correlation\n",
    "crop_avg_df = df.groupby(['Year', 'Commodity'])['Acre','Tons','None','D0',\t'D1','D2','D3','D4','pdsi','precip','temp'].mean().reset_index()\n",
    "crop_avg_df = crop_avg_df.pivot(index='Year', columns='Commodity', values='Tons').reset_index()\n",
    "crop_drought_corr = crop_avg_df.merge(avg_df[['Year', 'D3', 'pdsi', 'precip', 'temp']], on='Year')\n",
    "crop_drought_corr = crop_drought_corr.corr()\n",
    "crop_drought_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyIg_ielCEG4"
   },
   "outputs": [],
   "source": [
    "# display vis\n",
    "corrmatrix(crop_drought_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_drought_corr.sort_values('pdsi').index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this correlation matrix of each crop yields in tons to drought indicator, PDSI, we can see that the rank from least correlated are as follow.\n",
    "1. Sugarbeets\n",
    "2. Cotton\n",
    "3. Hay\n",
    "4. Corn\n",
    "5. Sugarcane\n",
    "6. Sorghum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Clustering of States Based on Variables\n",
    "using K-Means to explore how each states would be clustered if we were to based on PDSI, Temperature and Precipitation variables. We first need to explore what is the optimal number of clustering using elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the clustering of drought and climate in state regions using K-Means\n",
    "# first let's find optimal number of cluster using elbow methods\n",
    "\n",
    "# stardize the data\n",
    "df_clustering = df.copy()\n",
    "df_clustering = df[['State','pdsi', 'temp', 'precip']]\n",
    "X = StandardScaler().fit_transform(df_clustering.iloc[:, 1:])\n",
    "\n",
    "sse=[] # sum of square error\n",
    "list_k = list(range(1, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, init='k-means++', max_iter=300, random_state=0)\n",
    "    km.fit(X)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No definite optimal K cluster found using elbow method\n",
    "\n",
    "We need to find optimal cluster using silhouette score instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled features\n",
    "df_clustering = df.copy()\n",
    "df_clustering = df[['State','pdsi', 'temp', 'precip']]\n",
    "\n",
    "# removing outlier from df_clustering keep only the ones that are within +2.5 to -2.5 standard deviations in the column 'Data'.\n",
    "df_clustering[np.abs(df_clustering.pdsi-df_clustering.pdsi.mean()) <= (2.5*df_clustering.pdsi.std())]\n",
    "df_clustering[np.abs(df_clustering.temp-df_clustering.temp.mean()) <= (2.5*df_clustering.temp.std())]\n",
    "df_clustering[np.abs(df_clustering.precip-df_clustering.precip.mean()) <= (2.5*df_clustering.precip.std())]\n",
    "df_clustering\n",
    "\n",
    "# scale the data\n",
    "X = StandardScaler().fit_transform(df_clustering.iloc[:,1:])\n",
    "df_scaled = pd.DataFrame(X, columns=['pdsi', 'temp', 'precip'])\n",
    "\n",
    "# label encoding\n",
    "states = df_clustering.State.tolist()\n",
    "state_label = LabelEncoder().fit_transform(states)\n",
    "\n",
    "# merge label with scaled features\n",
    "df_scaled['state_label'] = state_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, k in enumerate([2, 3, 4, 5, 6]):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    \n",
    "    # Run the Kmeans algorithm\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, random_state=0)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    # centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Get silhouette samples\n",
    "    silhouette_vals = silhouette_samples(X, labels)\n",
    "\n",
    "    # Silhouette plot\n",
    "    y_ticks = []\n",
    "    y_lower, y_upper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        y_upper += len(cluster_silhouette_vals)\n",
    "        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "        ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "        y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "    # Get the average silhouette score and plot it\n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_xlabel('Silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster labels')\n",
    "    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n",
    "    \n",
    "    # Use MDS to flatten the data\n",
    "    embedding = MDS(n_components=2)\n",
    "    mds = pd.DataFrame(embedding.fit_transform(X), columns = ['component1','component2'])\n",
    "    mds['labels'] = kmeans.predict(X)\n",
    "\n",
    "    # Scatter plot of data colored with labels\n",
    "    ax2.scatter(mds['component1'], mds['component2'], c=labels)\n",
    "    # ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250)\n",
    "    # ax2.set_xlim([-2, 2])\n",
    "    # ax2.set_xlim([-2, 2])\n",
    "    ax2.set_xlabel('Component 1')\n",
    "    ax2.set_ylabel('Component 2')\n",
    "    ax2.set_title('Visualization of clustered data', y=1.02)\n",
    "    ax2.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Silhouette analysis using k = {k}',\n",
    "                 fontsize=16, fontweight='semibold', y=1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal K-Cluster for K-Means using Silhuette Score\n",
    "with the silhuette score chart above, with average mean being higher than other and not too few a cluster, k-clustering = 5 or 6 is good for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# using kmean clustering to find state cluster based on variables\n",
    "kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, random_state=0).fit(X)\n",
    "\n",
    "# sns.scatterplot(data=mds, x = \"component1\", y=\"component2\", hue=\"cluster\")\n",
    "\n",
    "df_scaled['clustering'] = kmeans.predict(X)\n",
    "df_scaled['State'] = df_clustering['State']\n",
    "df_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the clustering result with folium and geopandas\n",
    "uncomment the code below to plot the choropleth of the clustering by states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_group_gdf = pd.merge(df_scaled, geo_gdf, on='State', how='left')\n",
    "# gdf = gpd.GeoDataFrame(state_group_gdf)\n",
    "# m = folium.Map([43, -100], zoom_start=4, tiles='Stamen Terrain')\n",
    "# geo = gdf.explore(column='clustering', m=m, cmap='gnuplot', legend=True, scheme='EqualInterval', k=5)\n",
    "# geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hrO6fHuUCEG0"
   ],
   "name": "drought_crop.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "interpreter": {
   "hash": "467f60c46b88ea196e8e4e51716c4f14b05141d893e1660e0038b81da854c476"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
